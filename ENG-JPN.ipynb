{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79eca0a-d7f4-46b2-af7b-b3e0e88bc603",
      "metadata": {
        "id": "e79eca0a-d7f4-46b2-af7b-b3e0e88bc603"
      },
      "outputs": [],
      "source": [
        "!python3 -m venv venv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a9c9c0-1cef-41a2-96c6-f93d679ab9a0",
      "metadata": {
        "id": "a1a9c9c0-1cef-41a2-96c6-f93d679ab9a0"
      },
      "outputs": [],
      "source": [
        "!source venv/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a99a54-623b-455e-8879-d25add9c46f8",
      "metadata": {
        "id": "f6a99a54-623b-455e-8879-d25add9c46f8",
        "outputId": "0d48a131-76e8-4c4d-f62d-28c46cdf9eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS available: True\n",
            "MPS built: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())\n",
        "print(\"MPS built:\", torch.backends.mps.is_built())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c12750-d356-4d91-8d18-775384bd4dfd",
      "metadata": {
        "id": "40c12750-d356-4d91-8d18-775384bd4dfd",
        "outputId": "7665a409-70fb-4a3e-f8d0-594ff6bfbd78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.23.0)\n",
            "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1904b8-f505-4cfb-9a3b-f4edeeb85959",
      "metadata": {
        "id": "ea1904b8-f505-4cfb-9a3b-f4edeeb85959",
        "outputId": "a442367e-9905-4fa7-e23b-5e74e61c0ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet sentencepiece sacrebleu\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import math\n",
        "import random\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sentencepiece as spm\n",
        "import sacrebleu\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d7c06b-fce3-4e56-9a17-cf77117df21d",
      "metadata": {
        "id": "73d7c06b-fce3-4e56-9a17-cf77117df21d",
        "outputId": "8395dbd1-b1a4-4ca4-fbd9-3f72dac218d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZIP already present.\n",
            "Already extracted.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train.en': PosixPath('small_parallel_enja-master/train.en'),\n",
              " 'train.ja': PosixPath('small_parallel_enja-master/train.ja'),\n",
              " 'dev.en': PosixPath('small_parallel_enja-master/dev.en'),\n",
              " 'dev.ja': PosixPath('small_parallel_enja-master/dev.ja'),\n",
              " 'test.en': PosixPath('small_parallel_enja-master/test.en'),\n",
              " 'test.ja': PosixPath('small_parallel_enja-master/test.ja')}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATA_URL = \"https://github.com/odashi/small_parallel_enja/archive/refs/heads/master.zip\"\n",
        "ZIP_PATH = Path(\"small_parallel_enja.zip\")\n",
        "EXTRACT_DIR = Path(\"small_parallel_enja-master\")\n",
        "\n",
        "if not ZIP_PATH.exists():\n",
        "    print(\"Downloading dataset ZIP...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, ZIP_PATH)\n",
        "else:\n",
        "    print(\"ZIP already present.\")\n",
        "\n",
        "if not EXTRACT_DIR.exists():\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "        zf.extractall(\".\")\n",
        "else:\n",
        "    print(\"Already extracted.\")\n",
        "\n",
        "\n",
        "def find_split_files(root: Path):\n",
        "    candidates = list(root.rglob(\"*\"))\n",
        "    def pick(name):\n",
        "        for p in candidates:\n",
        "            if p.name == name:\n",
        "                return p\n",
        "        return None\n",
        "    files = {\n",
        "        \"train.en\": pick(\"train.en\"),\n",
        "        \"train.ja\": pick(\"train.ja\"),\n",
        "        \"dev.en\": pick(\"dev.en\") or pick(\"tune.en\"),\n",
        "        \"dev.ja\": pick(\"dev.ja\") or pick(\"tune.ja\"),\n",
        "        \"test.en\": pick(\"test.en\"),\n",
        "        \"test.ja\": pick(\"test.ja\"),\n",
        "    }\n",
        "    return files\n",
        "\n",
        "files = find_split_files(EXTRACT_DIR)\n",
        "files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd98233-07ee-44e2-b3d7-1b3331777f90",
      "metadata": {
        "id": "4fd98233-07ee-44e2-b3d7-1b3331777f90",
        "outputId": "5598d9ec-a268-4ce7-dd1a-f524acbd79fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train pairs: 50000\n",
            "Dev pairs:   500\n",
            "Test pairs:  500\n",
            "EN vocab: 4000 | JA vocab: 4000\n"
          ]
        }
      ],
      "source": [
        "PAD_ID, UNK_ID, BOS_ID, EOS_ID = 0, 1, 2, 3\n",
        "VOCAB_EN = 8000\n",
        "VOCAB_JA = 8000\n",
        "\n",
        "# Read text files\n",
        "def read_lines(path: Path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return [ln.strip() for ln in f]\n",
        "\n",
        "assert files[\"train.en\"] and files[\"train.ja\"], \"train.{en,ja} not found\"\n",
        "train_en = read_lines(files[\"train.en\"])\n",
        "train_ja = read_lines(files[\"train.ja\"])\n",
        "\n",
        "dev_en = read_lines(files[\"dev.en\"]) if files[\"dev.en\"] else []\n",
        "dev_ja = read_lines(files[\"dev.ja\"]) if files[\"dev.ja\"] else []\n",
        "test_en = read_lines(files[\"test.en\"]) if files[\"test.en\"] else []\n",
        "test_ja = read_lines(files[\"test.ja\"]) if files[\"test.ja\"] else []\n",
        "\n",
        "def aligned_pairs(src, tgt):\n",
        "    return [(s, t) for s, t in zip(src, tgt) if s and t]\n",
        "\n",
        "train_pairs = aligned_pairs(train_en, train_ja)\n",
        "dev_pairs = aligned_pairs(dev_en, dev_ja) if dev_en and dev_ja else []\n",
        "test_pairs = aligned_pairs(test_en, test_ja) if test_en and test_ja else []\n",
        "\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Dev pairs:   {len(dev_pairs)}\")\n",
        "print(f\"Test pairs:  {len(test_pairs)}\")\n",
        "\n",
        "def train_sp(sentences, prefix, vocab_size):\n",
        "    txt = Path(f\"{prefix}_spm_input.txt\")\n",
        "    txt.write_text(\"\\n\".join(sentences), encoding=\"utf-8\")\n",
        "    if not Path(f\"{prefix}.model\").exists():\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            (\n",
        "                f\"--input={txt} --model_prefix={prefix} --vocab_size={vocab_size} \"\n",
        "                f\"--pad_id={PAD_ID} --unk_id={UNK_ID} --bos_id={BOS_ID} --eos_id={EOS_ID} \"\n",
        "                \"--character_coverage=1.0 --input_sentence_size=2000000 \"\n",
        "                \"--shuffle_input_sentence=true --minloglevel=2\"\n",
        "            )\n",
        "        )\n",
        "    return spm.SentencePieceProcessor(model_file=f\"{prefix}.model\")\n",
        "\n",
        "\n",
        "en_sp = train_sp([p[0] for p in train_pairs], \"en\", VOCAB_EN)\n",
        "ja_sp = train_sp([p[1] for p in train_pairs], \"ja\", VOCAB_JA)\n",
        "\n",
        "print(\"EN vocab:\", en_sp.get_piece_size(), \"| JA vocab:\", ja_sp.get_piece_size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b619a6e8-5a36-45fe-bb16-e1356c6ea029",
      "metadata": {
        "id": "b619a6e8-5a36-45fe-bb16-e1356c6ea029",
        "outputId": "07ee17b5-c4f6-44c0-db8b-da9a7e66fbd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(782, 8, 8)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add_bos_eos(ids):\n",
        "    return [BOS_ID] + ids + [EOS_ID]\n",
        "\n",
        "class EnJaDataset(Dataset):\n",
        "    def __init__(self, pairs, en_sp, ja_sp, max_src_len=128, max_tgt_len=128):\n",
        "        self.pairs = pairs\n",
        "        self.en_sp = en_sp\n",
        "        self.ja_sp = ja_sp\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en, ja = self.pairs[idx]\n",
        "        src_ids = add_bos_eos(self.en_sp.encode(en, out_type=int))[: self.max_src_len]\n",
        "        tgt_ids = add_bos_eos(self.ja_sp.encode(ja, out_type=int))[: self.max_tgt_len]\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "def pad_sequences(seqs, pad=PAD_ID):\n",
        "    max_len = max(s.size(0) for s in seqs)\n",
        "    out = torch.full((len(seqs), max_len), pad, dtype=torch.long)\n",
        "    for i, s in enumerate(seqs):\n",
        "        out[i, : s.size(0)] = s\n",
        "    return out\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_list, tgt_list = zip(*batch)\n",
        "    src = pad_sequences(src_list, PAD_ID)\n",
        "    tgt = pad_sequences(tgt_list, PAD_ID)\n",
        "    tgt_inp = tgt[:, :-1]\n",
        "    tgt_out = tgt[:, 1:]\n",
        "\n",
        "    src_key_padding_mask = (src == PAD_ID)\n",
        "    tgt_key_padding_mask = (tgt_inp == PAD_ID)\n",
        "    T = tgt_inp.size(1)\n",
        "    subsequent = torch.triu(torch.ones((T, T), dtype=torch.bool), diagonal=1)\n",
        "    attn_mask = subsequent.unsqueeze(0)  # broadcast over batch & heads later\n",
        "    return src, tgt_inp, tgt_out, src_key_padding_mask, tgt_key_padding_mask, attn_mask\n",
        "\n",
        "\n",
        "MAX_TRAIN = None\n",
        "train_data = EnJaDataset(train_pairs[:MAX_TRAIN] if MAX_TRAIN else train_pairs, en_sp, ja_sp)\n",
        "val_data = EnJaDataset(dev_pairs if dev_pairs else train_pairs[:2000], en_sp, ja_sp)  # fallback if no dev\n",
        "test_data = EnJaDataset(test_pairs if test_pairs else train_pairs[-2000:], en_sp, ja_sp)  # fallback if no test\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
        "\n",
        "len(train_loader), len(val_loader), len(test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafd6a7c-4ebb-4ee3-9b6a-6a33ce258b28",
      "metadata": {
        "id": "cafd6a7c-4ebb-4ee3-9b6a-6a33ce258b28"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, L, D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, D)\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.o_proj = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None, key_padding_mask=None):\n",
        "        # q,k,v: (B, L, D)\n",
        "        B, Lq, _ = q.size()\n",
        "        B, Lk, _ = k.size()\n",
        "\n",
        "        def shape(x, L):\n",
        "            x = x.view(B, L, self.n_heads, self.d_k).transpose(1, 2)  # (B, H, L, d_k)\n",
        "            return x\n",
        "\n",
        "        Q = shape(self.q_proj(q), Lq)\n",
        "        K = shape(self.k_proj(k), Lk)\n",
        "        V = shape(self.v_proj(v), Lk)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (B, H, Lq, Lk)\n",
        "\n",
        "        # Add masks\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask.unsqueeze(1), float(\"-inf\"))\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.drop(attn)\n",
        "        context = torch.matmul(attn, V)  # (B, H, Lq, d_k)\n",
        "        context = context.transpose(1, 2).contiguous().view(B, Lq, self.d_model)  # (B, Lq, D)\n",
        "        out = self.o_proj(context)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.drop(F.gelu(self.lin1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None):\n",
        "        # Pre-norm\n",
        "        x = src\n",
        "        x2 = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x),\n",
        "                            attn_mask=None, key_padding_mask=src_key_padding_mask)\n",
        "        x = x + self.drop(x2)\n",
        "        x2 = self.ff(self.norm2(x))\n",
        "        x = x + self.drop(x2)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_attn_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        x = tgt\n",
        "        x2 = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x),\n",
        "                            attn_mask=tgt_attn_mask, key_padding_mask=tgt_key_padding_mask)\n",
        "        x = x + self.drop(x2)\n",
        "        x2 = self.cross_attn(self.norm2(x), memory, memory,\n",
        "                             attn_mask=None, key_padding_mask=memory_key_padding_mask)\n",
        "        x = x + self.drop(x2)\n",
        "        x2 = self.ff(self.norm3(x))\n",
        "        x = x + self.drop(x2)\n",
        "        return x\n",
        "\n",
        "class TransformerFromScratch(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=512, n_heads=8, num_enc=6, num_dec=6, d_ff=2048, dropout=0.1, max_len=4096):\n",
        "        super().__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab, d_model, padding_idx=PAD_ID)\n",
        "        self.pos = PositionalEncoding(d_model, max_len)\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_enc)])\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_dec)])\n",
        "        self.out = nn.Linear(d_model, tgt_vocab)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        # Init\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def encode(self, src, src_key_padding_mask):\n",
        "        x = self.drop(self.pos(self.src_emb(src)))\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_attn_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
        "        x = self.drop(self.pos(self.tgt_emb(tgt)))\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, memory, tgt_attn_mask=tgt_attn_mask,\n",
        "                      tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                      memory_key_padding_mask=memory_key_padding_mask)\n",
        "        return x\n",
        "\n",
        "    def forward(self, src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask, tgt_sub_mask):\n",
        "        memory = self.encode(src, src_key_padding_mask)  # (B, S, D)\n",
        "        dec = self.decode(tgt_inp, memory, tgt_sub_mask, tgt_key_padding_mask, src_key_padding_mask)  # (B, T, D)\n",
        "        logits = self.out(dec)  # (B, T, Vtgt)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13efed0e-9b50-462a-9d91-1016ff937f3c",
      "metadata": {
        "id": "13efed0e-9b50-462a-9d91-1016ff937f3c"
      },
      "outputs": [],
      "source": [
        "def noam_schedule(step, d_model, warmup):\n",
        "    step = max(step, 1)\n",
        "    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup ** -1.5))\n",
        "\n",
        "class NoamOpt:\n",
        "    def __init__(self, d_model, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        lr = noam_schedule(self._step, self.d_model, self.warmup)\n",
        "        for pg in self.optimizer.param_groups:\n",
        "            pg[\"lr\"] = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "def compute_loss(logits, targets):\n",
        "    # logits: (B,T,V), targets: (B,T)\n",
        "    return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=PAD_ID)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model, src, src_key_padding_mask, max_len=100):\n",
        "    model.eval()\n",
        "    B = src.size(0)\n",
        "    memory = model.encode(src, src_key_padding_mask)  # (B,S,D)\n",
        "    ys = torch.full((B, 1), BOS_ID, dtype=torch.long, device=src.device)\n",
        "    for _ in range(max_len - 1):\n",
        "        T = ys.size(1)\n",
        "        subsequent = torch.triu(torch.ones((T, T), dtype=torch.bool, device=src.device), diagonal=1)\n",
        "        logits = model.decode(ys, memory, subsequent.unsqueeze(0), ys.eq(PAD_ID), src_key_padding_mask)\n",
        "        next_token = model.out(logits[:, -1, :]).argmax(-1, keepdim=True)  # greedy\n",
        "        ys = torch.cat([ys, next_token], dim=1)\n",
        "        if (next_token == EOS_ID).all():  # all sequences ended\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def ids_to_text(ids_batch, sp):\n",
        "    outs = []\n",
        "    for ids in ids_batch:\n",
        "        ids = ids.tolist()\n",
        "        # strip BOS/EOS\n",
        "        if ids and ids[0] == BOS_ID: ids = ids[1:]\n",
        "        if ids and ids[-1] == EOS_ID: ids = ids[:-1]\n",
        "        outs.append(sp.decode(ids))\n",
        "    return outs\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bleu(model, loader, en_sp, ja_sp, max_len=100, limit_batches=None):\n",
        "    refs = []\n",
        "    hyps = []\n",
        "    for b, (src, tgt_inp, tgt_out, src_kpm, tgt_kpm, tgt_sub) in enumerate(loader):\n",
        "        src = src.to(device)\n",
        "        src_kpm = src_kpm.to(device)\n",
        "        pred_ids = greedy_decode(model, src, src_kpm, max_len=max_len)\n",
        "        hyps.extend(ids_to_text(pred_ids, ja_sp))\n",
        "        refs.extend([[r] for r in ids_to_text(tgt_out.to(device), ja_sp)])\n",
        "        if limit_batches and (b + 1) >= limit_batches:\n",
        "            break\n",
        "    return sacrebleu.corpus_bleu(hyps, refs).score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3dd2866-4662-4610-a9fd-fba9932ee122",
      "metadata": {
        "id": "e3dd2866-4662-4610-a9fd-fba9932ee122",
        "outputId": "53057240-f3f0-4966-f675-a3b30070dfd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Step 200/782 | Loss 5.9563\n",
            "Epoch 1 | Step 400/782 | Loss 5.3883\n",
            "Epoch 1 | Step 600/782 | Loss 5.0256\n",
            "Epoch 1 done. TrainLoss 4.7296 | Val BLEU 47.90\n",
            "✓ Saved checkpoint with BLEU = 47.89736254435747\n",
            "Epoch 2 | Step 200/782 | Loss 3.3514\n",
            "Epoch 2 | Step 400/782 | Loss 3.2190\n",
            "Epoch 2 | Step 600/782 | Loss 3.0986\n",
            "Epoch 2 done. TrainLoss 2.9953 | Val BLEU 9.31\n",
            "Epoch 3 | Step 200/782 | Loss 2.4745\n",
            "Epoch 3 | Step 400/782 | Loss 2.4222\n",
            "Epoch 3 | Step 600/782 | Loss 2.3705\n",
            "Epoch 3 done. TrainLoss 2.3211 | Val BLEU 10.50\n",
            "Epoch 4 | Step 200/782 | Loss 1.9854\n",
            "Epoch 4 | Step 400/782 | Loss 1.9584\n",
            "Epoch 4 | Step 600/782 | Loss 1.9274\n",
            "Epoch 4 done. TrainLoss 1.9033 | Val BLEU 16.28\n",
            "Epoch 5 | Step 200/782 | Loss 1.6587\n",
            "Epoch 5 | Step 400/782 | Loss 1.6595\n",
            "Epoch 5 | Step 600/782 | Loss 1.6484\n",
            "Epoch 5 done. TrainLoss 1.6393 | Val BLEU 8.62\n",
            "Epoch 6 | Step 200/782 | Loss 1.4548\n",
            "Epoch 6 | Step 400/782 | Loss 1.4626\n",
            "Epoch 6 | Step 600/782 | Loss 1.4626\n",
            "Epoch 6 done. TrainLoss 1.4608 | Val BLEU 8.19\n",
            "Epoch 7 | Step 200/782 | Loss 1.3124\n",
            "Epoch 7 | Step 400/782 | Loss 1.3235\n",
            "Epoch 7 | Step 600/782 | Loss 1.3310\n",
            "Epoch 7 done. TrainLoss 1.3300 | Val BLEU 5.51\n",
            "Epoch 8 | Step 200/782 | Loss 1.2022\n",
            "Epoch 8 | Step 400/782 | Loss 1.2145\n",
            "Epoch 8 | Step 600/782 | Loss 1.2238\n",
            "Epoch 8 done. TrainLoss 1.2294 | Val BLEU 2.35\n",
            "Epoch 9 | Step 200/782 | Loss 1.1072\n",
            "Epoch 9 | Step 400/782 | Loss 1.1284\n",
            "Epoch 9 | Step 600/782 | Loss 1.1386\n",
            "Epoch 9 done. TrainLoss 1.1425 | Val BLEU 9.03\n",
            "Epoch 10 | Step 200/782 | Loss 1.0277\n",
            "Epoch 10 | Step 400/782 | Loss 1.0515\n",
            "Epoch 10 | Step 600/782 | Loss 1.0655\n",
            "Epoch 10 done. TrainLoss 1.0729 | Val BLEU 11.93\n"
          ]
        }
      ],
      "source": [
        "SRC_VOCAB = en_sp.get_piece_size()\n",
        "TGT_VOCAB = ja_sp.get_piece_size()\n",
        "\n",
        "d_model = 256\n",
        "n_heads = 4\n",
        "num_enc = 4\n",
        "num_dec = 4\n",
        "d_ff = 1024\n",
        "dropout = 0.1\n",
        "warmup_steps = 2000\n",
        "MAX_EPOCHS = 10\n",
        "\n",
        "model = TransformerFromScratch(\n",
        "    SRC_VOCAB, TGT_VOCAB,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    num_enc=num_enc,\n",
        "    num_dec=num_dec,\n",
        "    d_ff=d_ff,\n",
        "    dropout=dropout,\n",
        "    max_len=1024\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=0.0\n",
        ")\n",
        "sched = NoamOpt(d_model, warmup_steps, opt)\n",
        "\n",
        "best_bleu = 0.0\n",
        "for epoch in range(1, MAX_EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (src, tgt_inp, tgt_out, src_kpm, tgt_kpm, tgt_sub) in enumerate(train_loader, 1):\n",
        "        src, tgt_inp, tgt_out = src.to(device), tgt_inp.to(device), tgt_out.to(device)\n",
        "        src_kpm, tgt_kpm = src_kpm.to(device), tgt_kpm.to(device)\n",
        "        tgt_sub = tgt_sub.to(device)\n",
        "\n",
        "        logits = model(src, tgt_inp, src_kpm, tgt_kpm, tgt_sub)\n",
        "        loss = compute_loss(logits, tgt_out)\n",
        "\n",
        "        sched.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        sched.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Epoch {epoch} | Step {i}/{len(train_loader)} | \"\n",
        "                  f\"Loss {total_loss / i:.4f}\")\n",
        "\n",
        "    bleu = evaluate_bleu(model, val_loader, en_sp, ja_sp, max_len=100, limit_batches=50)\n",
        "    print(f\"Epoch {epoch} done. TrainLoss {total_loss/len(train_loader):.4f} \"\n",
        "          f\"| Val BLEU {bleu:.2f}\")\n",
        "\n",
        "    if bleu > best_bleu:\n",
        "        best_bleu = bleu\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"cfg\": {\n",
        "                    \"d_model\": d_model,\n",
        "                    \"n_heads\": n_heads,\n",
        "                    \"num_enc\": num_enc,\n",
        "                    \"num_dec\": num_dec,\n",
        "                    \"d_ff\": d_ff,\n",
        "                    \"dropout\": dropout\n",
        "                }\n",
        "            },\n",
        "            \"enja_transformer.pt\"\n",
        "        )\n",
        "        print(\"✓ Saved checkpoint with BLEU =\", best_bleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b9cf14-31b5-45ca-a9d0-e77fb89725b7",
      "metadata": {
        "id": "61b9cf14-31b5-45ca-a9d0-e77fb89725b7"
      },
      "outputs": [],
      "source": [
        "def load_model(ckpt_path, SRC_VOCAB, TGT_VOCAB, device):\n",
        "    if not Path(ckpt_path).exists():\n",
        "        raise FileNotFoundError(f\"{ckpt_path} not found\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    cfg = ckpt[\"cfg\"]\n",
        "\n",
        "    model = TransformerFromScratch(\n",
        "        SRC_VOCAB, TGT_VOCAB,\n",
        "        d_model=cfg[\"d_model\"], n_heads=cfg[\"n_heads\"],\n",
        "        num_enc=cfg[\"num_enc\"], num_dec=cfg[\"num_dec\"],\n",
        "        d_ff=cfg[\"d_ff\"], dropout=cfg[\"dropout\"], max_len=2048\n",
        "    ).to(device)\n",
        "\n",
        "    state_dict = ckpt[\"model\"]\n",
        "    if state_dict[\"pos.pe\"].shape[1] != model.pos.pe.shape[1]:\n",
        "        old_pe = state_dict[\"pos.pe\"]\n",
        "        new_pe = torch.zeros_like(model.pos.pe)\n",
        "        length = min(old_pe.shape[1], new_pe.shape[1])\n",
        "        new_pe[:, :length, :] = old_pe[:, :length, :]\n",
        "        state_dict[\"pos.pe\"] = new_pe\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fdb20f1-f074-4d61-b0b2-7593df75f63d",
      "metadata": {
        "id": "5fdb20f1-f074-4d61-b0b2-7593df75f63d",
        "outputId": "e5998831-a8c9-40e7-ed7d-cb835f7c097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample translations:\n",
            "EN: Hello, hey\n",
            "JA: 彼 は 私 の を 見 て い る 。 が 。 が い 。 い 。 う 。 。 。\n",
            "\n",
            "EN: welcome\n",
            "JA: 私 は 部屋 に 行 い 。 で す 。 す 。 す 。 。 。 。 。 。\n",
            "\n",
            "EN: We are doing our best\n",
            "JA: この 本 は 何 の を 読 る 。 が な い 。 す 。 。 。 。 。 。\n",
            "\n",
            "EN: this is research assignment\n",
            "JA: この この 本 は この この この 本き で す 。 で す 。 す 。 。 。 。 。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checkpoint path\n",
        "ckpt_path = \"enja_transformer.pt\"\n",
        "\n",
        "if Path(ckpt_path).exists():\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    cfg = ckpt[\"cfg\"]\n",
        "\n",
        "    model = TransformerFromScratch(\n",
        "        SRC_VOCAB, TGT_VOCAB,\n",
        "        d_model=cfg[\"d_model\"], n_heads=cfg[\"n_heads\"],\n",
        "        num_enc=cfg[\"num_enc\"], num_dec=cfg[\"num_dec\"],\n",
        "        d_ff=cfg[\"d_ff\"], dropout=cfg[\"dropout\"], max_len=2048\n",
        "    ).to(device)\n",
        "\n",
        "    state_dict = ckpt[\"model\"]\n",
        "    if state_dict[\"pos.pe\"].shape[1] != model.pos.pe.shape[1]:\n",
        "        old_pe = state_dict[\"pos.pe\"]\n",
        "        new_pe = torch.zeros_like(model.pos.pe)\n",
        "        length = min(old_pe.shape[1], new_pe.shape[1])\n",
        "        new_pe[:, :length, :] = old_pe[:, :length, :]\n",
        "        state_dict[\"pos.pe\"] = new_pe\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "\n",
        "# Evaluate BLEU\n",
        "test_bleu = evaluate_bleu(model, test_loader, en_sp, ja_sp, max_len=100, limit_batches=50)\n",
        "\n",
        "# Translation function\n",
        "def translate_sentences(model, sentences, src_tokenizer, tgt_tokenizer, max_len=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = [\n",
        "            torch.tensor(add_bos_eos(src_tokenizer.encode(s, out_type=int)), dtype=torch.long)\n",
        "            for s in sentences\n",
        "        ]\n",
        "        src = pad_sequences(src_ids, PAD_ID).to(device)\n",
        "        src_kpm = (src == PAD_ID)\n",
        "        ys = greedy_decode(model, src, src_kpm.to(device), max_len=max_len)\n",
        "        return ids_to_text(ys, tgt_tokenizer)\n",
        "\n",
        "# Sample sentences\n",
        "samples = [\n",
        "    \"Hello, hey\",\n",
        "    \"welcome\",\n",
        "    \"We are doing our best\",\n",
        "    \"this is research assignment\"\n",
        "]\n",
        "\n",
        "print(\"\\nSample translations:\")\n",
        "for en, ja in zip(samples, translate_sentences(model, samples, en_sp, ja_sp, max_len=60)):\n",
        "    print(f\"EN: {en}\\nJA: {ja}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40944ba5-514d-4bbe-996a-f8fe792a984f",
      "metadata": {
        "id": "40944ba5-514d-4bbe-996a-f8fe792a984f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab10d56-4151-4406-a700-d703730e1389",
      "metadata": {
        "id": "cab10d56-4151-4406-a700-d703730e1389"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}